Установил NFS storage class  

```
The NFS Provisioner service has now been installed.

A storage class named 'nfs' has now been created
and is available to provision dynamic volumes.

You can use this storageclass by creating a `PersistentVolumeClaim` with the
correct storageClassName attribute. For example:

    ---
    kind: PersistentVolumeClaim
    apiVersion: v1
    metadata:
      name: test-dynamic-volume-claim
    spec:
      storageClassName: "nfs"
      accessModes:
        - ReadWriteOnce
      resources:
        requests:
          storage: 100Mi
 ```  
 
#### Задание 1: подключить для тестового конфига общую папку  
В stage окружении часто возникает необходимость отдавать статику бекенда сразу фронтом. Проще всего сделать это через общую папку. Требования:  
- в поде подключена общая папка между контейнерами (например, /static);  
- после записи чего-либо в контейнере с беком файлы можно получить из контейнера с фронтом.  

Использую манифест из лекции  
```  
iv_art@Pappa:~/kubespray/app/vols$ cat pod_vol.yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-int-volumes
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
        - mountPath: "/static"
          name: myvol
    - name: busybox
      image: busybox
      command: ["sleep", "3600"]
      volumeMounts:
        - mountPath: "/tmp/cache"
          name: myvol
  volumes:
    - name: myvol
      emptyDir: {}
iv_art@Pappa:~/kubespray/app/vols$
```  
Cтарт  
```  
iv_art@Pappa:~/kubespray/app/vols$
kubectl apply -f pod_vol.yaml
pod/pod-int-volumes created
iv_art@Pappa:~/kubespray/app/vols$ kubectl get pods,pv,pvc
NAME                                      READY   STATUS    RESTARTS         AGE
pod/backend-prod-8579d447b8-f65zc         1/1     Running   133 (8m1s ago)   22h
pod/frontend-prod-7f544d56b7-945zm        1/1     Running   0                22h
pod/hello-world-7c8458888d-xcs54          1/1     Running   0                22h
pod/nfs-server-nfs-server-provisioner-0   1/1     Running   0                18m
pod/pod-int-volumes                       2/2     Running   0                26s
pod/postgres-0                            1/1     Running   0                22h
```  
Пишу в контейнер  
```  
iv_art@Pappa:~/kubespray/app/vols$ kubectl exec pod-int-volumes -c busybox -- sh -c 'echo "test" > /tmp/cache/test.txt'
```  

Проверяем
```  
iv_art@Pappa:~/kubespray/app/vols$ kubectl exec pod-int-volumes -c busybox -- ls -la /tmp/cache
total 12
drwxrwxrwx    2 root     root          4096 Jun 23 03:40 .
drwxrwxrwt    1 root     root          4096 Jun 23 03:38 ..
-rw-r--r--    1 root     root             5 Jun 23 03:40 test.txt
```  
Проверяем из другого контейнера  
![image](https://user-images.githubusercontent.com/87374285/175204715-a03667db-1e98-4656-a3f2-934f90cec9fd.png)  

#### Задание 2: подключить общую папку для прода  
Поработав на stage, доработки нужно отправить на прод. В продуктиве у нас контейнеры крутятся в разных подах, поэтому потребуется PV и связь через PVC.  
Сам PV должен быть связан с NFS сервером. Требования:  
- все бекенды подключаются к одному PV в режиме ReadWriteMany;  
- фронтенды тоже подключаются к этому же PV с таким же режимом;  
- файлы, созданные бекендом, должны быть доступны фронту.  

файлы   
```
iv_art@Pappa:~/kubespray/app/vols$ cat pvc.yaml
---
kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: nfs
spec:
  storageClassName: "nfs"
  accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 100Mi

#Запуск

iv_art@Pappa:~/kubespray/app/vols$ kubectl apply -f pvc.yaml
persistentvolumeclaim/test-dpvc created
iv_art@Pappa:~/kubespray/app/vols$ kubectl get pvc
NAME        STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
test-dpvc   Bound    data     100Mi      RWX            nfs            14s
iv_art@Pappa:~/kubespray/app/vols$ kubectl get pv
NAME   CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
data   100Mi      RWX            Retain           Bound    default/test-dpvc   nfs                     29s
```  
Frontend из 13/1 
```  
iv_art@Pappa:~/kubespray/app/vols$ cat front2.yaml
# Front pod 4 prod
apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend-prod
  labels:
    app: frontend-prod
  namespace: default
spec:
  replicas: 1
  selector:
    matchLabels:
      app: frontend-prod
  template:
    metadata:
      labels:
        app: frontend-prod
    spec:
      containers:
      - name: client
        image: nginx:1.19
        ports:
        - containerPort: 80
        env:
        - name: BASE_URL
          value: backend-prod:9000
        volumeMounts:
        - name: nfs
          mountPath: /data
      volumes:
        - name: nfs
          persistentVolumeClaim:
            claimName: nfs

---
apiVersion: v1
kind: Service
metadata:
  name: frontend-prod
  namespace: default
spec:
  selector:
    app: frontend-prod
  ports:
    - name: frontend-prod
      protocol: TCP
      port: 8000
      targetPort: 80
  type: ClusterIP
```  
Backend из 13/1  
```  
iv_art@Pappa:~/kubespray/app/vols$ cat back2.yaml
# Config Stage env
apiVersion: apps/v1
kind: Deployment
metadata:
  name: backend-prod
  labels:
    app: backend-prod-app
spec:
  selector:
    matchLabels:
      app: backend-prod-app
  template:
    metadata:
      labels:
        app: backend-prod-app
    spec:
    # Config Containers
      containers:
      - name: backend
        image: debian
        command: ["sleep", "600"]
        env:
          - name: DATABASE_URL
            value: postgres://postgres:postgres@postgres:5432/news # адрес сервиса БД
        volumeMounts:
        - name: nfs
          mountPath: /data
      volumes:
        - name: nfs
          persistentVolumeClaim:
            claimName: nfs
---
# Config Service
apiVersion: v1
kind: Service
metadata:
  name: backend-prod-svc
  namespace: default
spec:
  selector:
    app: backend-prod-app
  ports:
    - name: backend-prod
      protocol: TCP
      port: 9000
      targetPort: 9000
```  

Получившаяся структура  
![image](https://user-images.githubusercontent.com/87374285/175239819-d14ceab2-1ee9-4ede-af76-608afaaeee50.png)

Проверяем каталог с обеих нод, пишем файл из frontend и опять проверяем все  
![image](https://user-images.githubusercontent.com/87374285/175239697-2e7ef1b3-b568-407b-adfb-90119c9d5449.png)

 

